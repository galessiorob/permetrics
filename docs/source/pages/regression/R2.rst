R2 - Coefficient of Determination
=================================

.. toctree::
   :maxdepth: 3
   :caption: R2 - Coefficient of Determination

.. toctree::
   :maxdepth: 3

.. toctree::
   :maxdepth: 3

.. toctree::
   :maxdepth: 3


.. math::

	R2(y, \hat{y}) = 1 - \frac{\sum_{i=1}^{N} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{N} (y_i - \bar{y})^2}

where :math:`\bar{y} = \frac{1}{N} \sum_{i=1}^{N} y_i` and :math:`\sum_{i=1}^{N} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{N} \epsilon_i^2`

The r2_score function computes the coefficient of determination, should denoted as R2.

It represents the proportion of variance (of y) that has been explained by the independent variables in the model.  It provides an indication of goodness of
fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance.

As such variance is dataset dependent, R2 may not be meaningfully comparable across different datasets.  Best possible score is 1.0 and it can be negative
(because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features,  would get a RÂ²
score of 0.0.

+ Best possible score is 1.0, bigger value is better. Range = (-inf, 1]

Latex equation code::

	\text{RMSE}(y, \hat{y}) = \sqrt{\frac{\sum_{i=0}^{N - 1} (y_i - \hat{y}_i)^2}{N}}


Example to use R2 metric:

.. code-block:: python
	:emphasize-lines: 8-9,15-16

	from numpy import array
	from permetrics.regression import RegressionMetric

	## For 1-D array
	y_true = array([3, -0.5, 2, 7])
	y_pred = array([2.5, 0.0, 2, 8])

	evaluator = RegressionMetric(y_true, y_pred, decimal=5)
	print(evaluator.pearson_correlation_coefficient())

	## For > 1-D array
	y_true = array([[0.5, 1], [-1, 1], [7, -6]])
	y_pred = array([[0, 2], [-1, 2], [8, -5]])

	evaluator = RegressionMetric(y_true, y_pred, decimal=5)
	print(evaluator.R(multi_output="raw_values"))

